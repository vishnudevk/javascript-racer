{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b> GAME ENVIRONMENT CODE & BASIC FUNCTIONS</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from game import Game\n",
    "from racing_env import RaceGameEnv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "source": [
    "for i in range(1):\n",
    "\n",
    "    img = game.takess()\n",
    "    print(img.shape)\n",
    "    im = Image.fromarray(img, 'RGB')    \n",
    "    imgplot = plt.imshow(im,cmap=plt.cm.binary)\n",
    "    #plt.show()\n",
    "\n",
    "    #im.save(\"screenshot.jpeg\")\n",
    "    game.getSpead()\n",
    "    game.move('up')\n",
    "\n",
    "game.resetGame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b> TENSORFLOW ENVIRONMENT CODE </b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "serving at port 8000\n127.0.0.1 - - [20/Jun/2020 03:27:12] \"GET /v4.final.noCars.html HTTP/1.1\" 200 -\n127.0.0.1 - - [20/Jun/2020 03:27:12] \"GET /common.css HTTP/1.1\" 200 -\n127.0.0.1 - - [20/Jun/2020 03:27:12] \"GET /stats.js HTTP/1.1\" 200 -\n127.0.0.1 - - [20/Jun/2020 03:27:12] \"GET /common.js HTTP/1.1\" 200 -\n127.0.0.1 - - [20/Jun/2020 03:27:12] \"GET /music/racer.ogg HTTP/1.1\" 200 -\n----------------------------------------\nException happened during processing of request from ('127.0.0.1', 62710)\nTraceback (most recent call last):\n  File \"c:\\program files\\python37\\Lib\\socketserver.py\", line 316, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"c:\\program files\\python37\\Lib\\socketserver.py\", line 347, in process_request\n    self.finish_request(request, client_address)\n  File \"c:\\program files\\python37\\Lib\\socketserver.py\", line 360, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"c:\\program files\\python37\\Lib\\http\\server.py\", line 646, in __init__\n    super().__init__(*args, **kwargs)\n  File \"c:\\program files\\python37\\Lib\\socketserver.py\", line 720, in __init__\n    self.handle()\n  File \"c:\\program files\\python37\\Lib\\http\\server.py\", line 426, in handle\n    self.handle_one_request()\n  File \"c:\\program files\\python37\\Lib\\http\\server.py\", line 414, in handle_one_request\n    method()\n  File \"c:\\program files\\python37\\Lib\\http\\server.py\", line 653, in do_GET\n    self.copyfile(f, self.wfile)\n  File \"c:\\program files\\python37\\Lib\\http\\server.py\", line 844, in copyfile\n    shutil.copyfileobj(source, outputfile)\n  File \"C:\\Users\\VishnudevK\\Desktop\\Tensorflow-Training\\tfpython\\lib\\shutil.py\", line 82, in copyfileobj\n    fdst.write(buf)\n  File \"c:\\program files\\python37\\Lib\\socketserver.py\", line 799, in write\n    self._sock.sendall(b)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n----------------------------------------\n127.0.0.1 - - [20/Jun/2020 03:27:28] \"GET /images/mute.png HTTP/1.1\" 200 -\n127.0.0.1 - - [20/Jun/2020 03:27:28] \"GET /images/background.png HTTP/1.1\" 200 -\n127.0.0.1 - - [20/Jun/2020 03:27:28] \"GET /images/sprites.png HTTP/1.1\" 200 -\nINIT IS TRIGGERED\n"
    }
   ],
   "source": [
    "env = RaceGameEnv()\n",
    "#env = tf_py_environment.TFPyEnvironment(env)\n",
    "#utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "127.0.0.1 - - [20/Jun/2020 03:27:29] code 404, message File not found\n127.0.0.1 - - [20/Jun/2020 03:27:29] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
    }
   ],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 500  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-8  # @param {type:\"number\"}\n",
    "log_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 500  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fc_layer_params = (200,)\n",
    "fc_layer_params = [100,50] #(150,75)\n",
    "\n",
    "##q_net = q_network.QNetwork(\n",
    "#    env.observation_spec(),\n",
    "#    env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)\n",
    "\n",
    "conv_layer_params = [( 16 , ( 3 , 3 ), 1 ), ( 16 , ( 3 , 3 ), 1 )]\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer QNetwork is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nWARNING:tensorflow:Layer TargetQNetwork is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n"
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Return from Random agent is -999\n"
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "env.action_spec())\n",
    "\n",
    "random_return = -999\n",
    "#random_return = compute_avg_return(env, random_policy, num_eval_episodes)\n",
    "\n",
    "env._reset()\n",
    "print(\"Return from Random agent is \" + str(random_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('step_type',\n 'observation',\n 'action',\n 'policy_info',\n 'next_step_type',\n 'reward',\n 'discount')"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  #start_time = time.time()\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "  #print(\"All collect_step took %s seconds\" % (time.time() - start_time))\n",
    "  \n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for i in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 55, 240, 3), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.float64, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002CAF6092668>\n"
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Agent training for 0 th time\nAgent trained in 97.52598857879639 seconds\nAgent training for 1 th time\nAgent trained in 98.84099531173706 seconds\nstep = 100: loss = 28.797378540039062\nAgent training for 2 th time\nAgent trained in 113.4559907913208 seconds\nAgent training for 3 th time\nAgent trained in 91.87411618232727 seconds\nstep = 200: loss = 21.260915756225586\nAgent training for 4 th time\nAgent trained in 92.23999977111816 seconds\nAgent training for 5 th time\nAgent trained in 91.18600916862488 seconds\nstep = 300: loss = 23.112892150878906\nAgent training for 6 th time\nAgent trained in 93.95199823379517 seconds\nAgent training for 7 th time\nAgent trained in 94.1770007610321 seconds\nstep = 400: loss = 35.135616302490234\nAgent training for 8 th time\nAgent trained in 90.58199906349182 seconds\nAgent training for 9 th time\nAgent trained in 82.61099410057068 seconds\nstep = 500: loss = 30.910812377929688\nstep = 500: Average Return = 24.200000762939453\nAgent training for 10 th time\nAgent trained in 85.71899437904358 seconds\nAgent training for 11 th time\nAgent trained in 82.48297452926636 seconds\nstep = 600: loss = 71.92456817626953\nAgent training for 12 th time\nAgent trained in 77.66502046585083 seconds\nAgent training for 13 th time\nAgent trained in 77.7019956111908 seconds\nstep = 700: loss = 37.634769439697266\nAgent training for 14 th time\nAgent trained in 78.56497597694397 seconds\nAgent training for 15 th time\nAgent trained in 77.69399929046631 seconds\nstep = 800: loss = 33.70133590698242\nAgent training for 16 th time\nAgent trained in 82.89000058174133 seconds\nAgent training for 17 th time\nAgent trained in 78.6099591255188 seconds\nstep = 900: loss = 38.774234771728516\nAgent training for 18 th time\nAgent trained in 79.44200778007507 seconds\nAgent training for 19 th time\nAgent trained in 79.08399701118469 seconds\nstep = 1000: loss = 44.50347137451172\nstep = 1000: Average Return = 109.9000015258789\nAgent training for 20 th time\nAgent trained in 80.26500129699707 seconds\nAgent training for 21 th time\nAgent trained in 81.6010000705719 seconds\nstep = 1100: loss = 45.54386901855469\nAgent training for 22 th time\nAgent trained in 84.08499789237976 seconds\nAgent training for 23 th time\nAgent trained in 81.51600694656372 seconds\nstep = 1200: loss = 44.908958435058594\nAgent training for 24 th time\nAgent trained in 77.18194246292114 seconds\nAgent training for 25 th time\nAgent trained in 76.36796760559082 seconds\nstep = 1300: loss = 42.69383239746094\nAgent training for 26 th time\nAgent trained in 77.77899813652039 seconds\nAgent training for 27 th time\nAgent trained in 80.3100004196167 seconds\nstep = 1400: loss = 54.940223693847656\nAgent training for 28 th time\nAgent trained in 82.33494973182678 seconds\nAgent training for 29 th time\nAgent trained in 78.88599586486816 seconds\nstep = 1500: loss = 44.70722961425781\nstep = 1500: Average Return = 124.30000305175781\nAgent training for 30 th time\nAgent trained in 80.19399642944336 seconds\nAgent training for 31 th time\nAgent trained in 81.29900121688843 seconds\nstep = 1600: loss = 46.7930908203125\nAgent training for 32 th time\nAgent trained in 81.75899934768677 seconds\nAgent training for 33 th time\nAgent trained in 80.94300198554993 seconds\nstep = 1700: loss = 46.6750373840332\nAgent training for 34 th time\nAgent trained in 80.77998566627502 seconds\nAgent training for 35 th time\nAgent trained in 78.30695462226868 seconds\nstep = 1800: loss = 42.31645202636719\nAgent training for 36 th time\nAgent trained in 77.61300039291382 seconds\nAgent training for 37 th time\nAgent trained in 79.70999813079834 seconds\nstep = 1900: loss = 48.511566162109375\nAgent training for 38 th time\nAgent trained in 81.55899906158447 seconds\nAgent training for 39 th time\nAgent trained in 80.0999596118927 seconds\nstep = 2000: loss = 49.78971862792969\nstep = 2000: Average Return = 103.5999984741211\nAgent training for 40 th time\nAgent trained in 81.8849949836731 seconds\nAgent training for 41 th time\nAgent trained in 77.71499514579773 seconds\nstep = 2100: loss = 47.541175842285156\nAgent training for 42 th time\nAgent trained in 75.99695658683777 seconds\nAgent training for 43 th time\nAgent trained in 78.16899967193604 seconds\nstep = 2200: loss = 48.38547897338867\nAgent training for 44 th time\nAgent trained in 82.27196407318115 seconds\nAgent training for 45 th time\nAgent trained in 81.96395444869995 seconds\nstep = 2300: loss = 49.789161682128906\nAgent training for 46 th time\nAgent trained in 81.98301720619202 seconds\nAgent training for 47 th time\nAgent trained in 80.40999984741211 seconds\nstep = 2400: loss = 47.37126541137695\nAgent training for 48 th time\nAgent trained in 78.11096119880676 seconds\nAgent training for 49 th time\nAgent trained in 78.6459596157074 seconds\nstep = 2500: loss = 49.366851806640625\nstep = 2500: Average Return = 86.19999694824219\nAgent training for 50 th time\nAgent trained in 80.21299576759338 seconds\nAgent training for 51 th time\nAgent trained in 78.99499821662903 seconds\nstep = 2600: loss = 49.95313262939453\nAgent training for 52 th time\nAgent trained in 78.65599966049194 seconds\nAgent training for 53 th time\nAgent trained in 79.06600451469421 seconds\nstep = 2700: loss = 51.557918548583984\nAgent training for 54 th time\nAgent trained in 80.56199789047241 seconds\nAgent training for 55 th time\nAgent trained in 84.07695293426514 seconds\nstep = 2800: loss = 50.792762756347656\nAgent training for 56 th time\nAgent trained in 81.64898681640625 seconds\nAgent training for 57 th time\nAgent trained in 81.03599834442139 seconds\nstep = 2900: loss = 48.943397521972656\nAgent training for 58 th time\nAgent trained in 78.13799476623535 seconds\nAgent training for 59 th time\nAgent trained in 80.29999494552612 seconds\nstep = 3000: loss = 47.52151107788086\nstep = 3000: Average Return = 66.69999694824219\nAgent training for 60 th time\nAgent trained in 84.59800410270691 seconds\nAgent training for 61 th time\nAgent trained in 80.42296075820923 seconds\nstep = 3100: loss = 45.66383361816406\nAgent training for 62 th time\nAgent trained in 77.48600006103516 seconds\nAgent training for 63 th time\nAgent trained in 79.36900234222412 seconds\nstep = 3200: loss = 54.101680755615234\nAgent training for 64 th time\nAgent trained in 76.47099685668945 seconds\nAgent training for 65 th time\nAgent trained in 78.91100239753723 seconds\nstep = 3300: loss = 47.25722885131836\nAgent training for 66 th time\nAgent trained in 75.45599961280823 seconds\nAgent training for 67 th time\nAgent trained in 77.39400029182434 seconds\nstep = 3400: loss = 48.8807373046875\nAgent training for 68 th time\nAgent trained in 76.9759874343872 seconds\nAgent training for 69 th time\nAgent trained in 79.45399737358093 seconds\nstep = 3500: loss = 46.30620193481445\nstep = 3500: Average Return = 85.19999694824219\nAgent training for 70 th time\nAgent trained in 81.03900122642517 seconds\nAgent training for 71 th time\nAgent trained in 79.92599582672119 seconds\nstep = 3600: loss = 50.5406494140625\nAgent training for 72 th time\nAgent trained in 80.37599968910217 seconds\nAgent training for 73 th time\nAgent trained in 79.44495677947998 seconds\nstep = 3700: loss = 47.15788269042969\nAgent training for 74 th time\nAgent trained in 79.6969940662384 seconds\nAgent training for 75 th time\nAgent trained in 79.51299667358398 seconds\nstep = 3800: loss = 48.7861328125\nAgent training for 76 th time\nAgent trained in 79.96998691558838 seconds\nAgent training for 77 th time\nAgent trained in 89.39899969100952 seconds\nstep = 3900: loss = 49.960975646972656\nAgent training for 78 th time\nAgent trained in 87.32199716567993 seconds\nAgent training for 79 th time\nAgent trained in 80.16399908065796 seconds\nstep = 4000: loss = 49.30291748046875\nstep = 4000: Average Return = 148.39999389648438\nAgent training for 80 th time\nAgent trained in 80.11399269104004 seconds\nAgent training for 81 th time\nAgent trained in 78.71598267555237 seconds\nstep = 4100: loss = 50.080047607421875\nAgent training for 82 th time\nAgent trained in 79.03395199775696 seconds\nAgent training for 83 th time\nAgent trained in 78.58799386024475 seconds\nstep = 4200: loss = 50.2902717590332\nAgent training for 84 th time\nAgent trained in 79.42899870872498 seconds\nAgent training for 85 th time\nAgent trained in 79.39899945259094 seconds\nstep = 4300: loss = 49.645179748535156\nAgent training for 86 th time\nAgent trained in 79.36599469184875 seconds\nAgent training for 87 th time\nAgent trained in 79.92499852180481 seconds\nstep = 4400: loss = 45.526947021484375\nAgent training for 88 th time\nAgent trained in 83.23399758338928 seconds\nAgent training for 89 th time\nAgent trained in 81.43597841262817 seconds\nstep = 4500: loss = 49.2337760925293\nstep = 4500: Average Return = 69.80000305175781\nAgent training for 90 th time\nAgent trained in 80.54298329353333 seconds\nAgent training for 91 th time\nAgent trained in 80.90096116065979 seconds\nstep = 4600: loss = 48.064552307128906\nAgent training for 92 th time\nAgent trained in 80.83099913597107 seconds\nAgent training for 93 th time\nAgent trained in 79.64599704742432 seconds\nstep = 4700: loss = 48.95973587036133\nAgent training for 94 th time\nAgent trained in 78.84099698066711 seconds\nAgent training for 95 th time\nAgent trained in 80.0919623374939 seconds\nstep = 4800: loss = 47.93077850341797\nAgent training for 96 th time\nAgent trained in 77.97200655937195 seconds\nAgent training for 97 th time\nAgent trained in 77.66096186637878 seconds\nstep = 4900: loss = 45.83355712890625\nAgent training for 98 th time\nAgent trained in 77.80995059013367 seconds\nAgent training for 99 th time\nAgent trained in 78.11300706863403 seconds\nstep = 5000: loss = 40.46574401855469\nstep = 5000: Average Return = 134.1999969482422\nAgent training for 100 th time\nAgent trained in 78.77902054786682 seconds\nAgent training for 101 th time\nAgent trained in 81.90799713134766 seconds\nstep = 5100: loss = 51.17299270629883\nAgent training for 102 th time\nAgent trained in 81.17899894714355 seconds\nAgent training for 103 th time\nAgent trained in 79.82298803329468 seconds\nstep = 5200: loss = 49.45262145996094\nAgent training for 104 th time\nAgent trained in 80.91900253295898 seconds\nAgent training for 105 th time\nAgent trained in 80.80899262428284 seconds\nstep = 5300: loss = 48.81422424316406\nAgent training for 106 th time\nAgent trained in 79.20300030708313 seconds\nAgent training for 107 th time\nAgent trained in 79.15099930763245 seconds\nstep = 5400: loss = 51.92303466796875\nAgent training for 108 th time\nAgent trained in 81.43297410011292 seconds\nAgent training for 109 th time\nAgent trained in 82.21396088600159 seconds\nstep = 5500: loss = 49.351688385009766\nstep = 5500: Average Return = 159.8000030517578\nAgent training for 110 th time\nAgent trained in 78.26699733734131 seconds\nAgent training for 111 th time\nAgent trained in 78.80599570274353 seconds\nstep = 5600: loss = 46.19190216064453\nAgent training for 112 th time\nAgent trained in 79.75694561004639 seconds\nAgent training for 113 th time\nAgent trained in 78.7269446849823 seconds\nstep = 5700: loss = 53.69324493408203\nAgent training for 114 th time\nAgent trained in 79.08099699020386 seconds\nAgent training for 115 th time\nAgent trained in 79.90498733520508 seconds\nstep = 5800: loss = 50.20648956298828\nAgent training for 116 th time\nAgent trained in 79.2159595489502 seconds\nAgent training for 117 th time\nAgent trained in 79.54799771308899 seconds\nstep = 5900: loss = 47.09034729003906\nAgent training for 118 th time\nAgent trained in 78.44099640846252 seconds\nAgent training for 119 th time\nAgent trained in 80.71899485588074 seconds\nstep = 6000: loss = 49.670745849609375\nstep = 6000: Average Return = 68.0999984741211\nAgent training for 120 th time\nAgent trained in 82.55896329879761 seconds\nAgent training for 121 th time\nAgent trained in 80.49201011657715 seconds\nstep = 6100: loss = 51.15060806274414\nAgent training for 122 th time\nAgent trained in 83.93899202346802 seconds\nAgent training for 123 th time\nAgent trained in 79.11799693107605 seconds\nstep = 6200: loss = 47.3980827331543\nAgent training for 124 th time\nAgent trained in 78.49799919128418 seconds\nAgent training for 125 th time\nAgent trained in 77.03799295425415 seconds\nstep = 6300: loss = 48.58331298828125\nAgent training for 126 th time\nAgent trained in 78.96398448944092 seconds\nAgent training for 127 th time\nAgent trained in 78.39698100090027 seconds\nstep = 6400: loss = 47.830928802490234\nAgent training for 128 th time\nAgent trained in 79.491952419281 seconds\nAgent training for 129 th time\nAgent trained in 77.05799698829651 seconds\nstep = 6500: loss = 52.82572555541992\nstep = 6500: Average Return = 102.5999984741211\nAgent training for 130 th time\nAgent trained in 81.042964220047 seconds\nAgent training for 131 th time\nAgent trained in 78.4549970626831 seconds\nstep = 6600: loss = 47.452537536621094\nAgent training for 132 th time\nAgent trained in 77.79599928855896 seconds\nAgent training for 133 th time\nAgent trained in 76.90495443344116 seconds\nstep = 6700: loss = 52.65261459350586\nAgent training for 134 th time\nAgent trained in 76.83599400520325 seconds\nAgent training for 135 th time\nAgent trained in 77.39699816703796 seconds\nstep = 6800: loss = 47.55461120605469\nAgent training for 136 th time\nAgent trained in 86.45886516571045 seconds\nAgent training for 137 th time\nAgent trained in 97.05899953842163 seconds\nstep = 6900: loss = 49.502925872802734\nAgent training for 138 th time\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-821d9f0393b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mexperience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Agent trained in %s seconds\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m   \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Desktop\\Tensorflow-Training\\tfpython\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "env._reset()\n",
    "avg_return = 0\n",
    "#avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for j in range(collect_steps_per_iteration):\n",
    "    #print(\"Step \" + str(j))\n",
    "    collect_step(env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  env._reset()\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  print(\"Agent training for %s th time\" % str(i))\n",
    "  start_time = time.time()\n",
    "  for j in range(collect_steps_per_iteration):\n",
    "    experience, unused_info = next(iterator)  \n",
    "    \n",
    "    train_loss = agent.train(experience).loss\n",
    "  print(\"Agent trained in %s seconds\" % (time.time() - start_time))\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    env._reset()\n",
    "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "106.1"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "\n",
    "avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bittfpythonvirtualenvc896718c15ce430fbcbfe7334c839c00",
   "display_name": "Python 3.7.3 64-bit ('tfpython': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}