{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b> GAME ENVIRONMENT CODE & BASIC FUNCTIONS</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from game import Game\n",
    "from racing_env import RaceGameEnv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "source": [
    "for i in range(1):\n",
    "\n",
    "    img = game.takess()\n",
    "    print(img.shape)\n",
    "    im = Image.fromarray(img, 'RGB')    \n",
    "    imgplot = plt.imshow(im,cmap=plt.cm.binary)\n",
    "    #plt.show()\n",
    "\n",
    "    #im.save(\"screenshot.jpeg\")\n",
    "    game.getSpead()\n",
    "    game.move('up')\n",
    "\n",
    "game.resetGame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b> TENSORFLOW ENVIRONMENT CODE </b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = RaceGameEnv()\n",
    "#env = tf_py_environment.TFPyEnvironment(env)\n",
    "#utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 500  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 50  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-8  # @param {type:\"number\"}\n",
    "log_interval = 100  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 300  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fc_layer_params = (200,)\n",
    "fc_layer_params = [100,50] #(150,75)\n",
    "\n",
    "##q_net = q_network.QNetwork(\n",
    "#    env.observation_spec(),\n",
    "#    env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)\n",
    "\n",
    "conv_layer_params = [( 16 , ( 3 , 3 ), 1 ), ( 16 , ( 3 , 3 ), 1 )]\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "env.action_spec())\n",
    "\n",
    "random_return = -999\n",
    "#random_return = compute_avg_return(env, random_policy, num_eval_episodes)\n",
    "\n",
    "env._reset()\n",
    "print(\"Return from Random agent is \" + str(random_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  #start_time = time.time()\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "  #print(\"All collect_step took %s seconds\" % (time.time() - start_time))\n",
    "  \n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for i in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "env._reset()\n",
    "avg_return = 0\n",
    "#avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for j in range(collect_steps_per_iteration):\n",
    "    #print(\"Step \" + str(j))\n",
    "    collect_step(env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  env._reset()\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  print(\"Agent training for %s th time\" % str(i))\n",
    "  start_time = time.time()\n",
    "  for j in range(collect_steps_per_iteration):\n",
    "    experience, unused_info = next(iterator)  \n",
    "    \n",
    "    train_loss = agent.train(experience).loss\n",
    "  print(\"Agent trained in %s seconds\" % (time.time() - start_time))\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    env._reset()\n",
    "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bittfpythonvirtualenvc896718c15ce430fbcbfe7334c839c00",
   "display_name": "Python 3.7.3 64-bit ('tfpython': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}