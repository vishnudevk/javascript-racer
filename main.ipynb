{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b> GAME ENVIRONMENT CODE & BASIC FUNCTIONS</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "from game import Game\n",
    "from racing_env import RaceGameEnv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "source": [
    "for i in range(1):\n",
    "\n",
    "    img = game.takess()\n",
    "    print(img.shape)\n",
    "    im = Image.fromarray(img, 'RGB')    \n",
    "    imgplot = plt.imshow(im,cmap=plt.cm.binary)\n",
    "    #plt.show()\n",
    "\n",
    "    #im.save(\"screenshot.jpeg\")\n",
    "    game.getSpead()\n",
    "    game.move('up')\n",
    "\n",
    "game.resetGame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b> TENSORFLOW ENVIRONMENT CODE </b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "serving at port 8000\n127.0.0.1 - - [22/Jun/2020 16:58:35] \"GET /v4.final.noCars.html HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:35] \"GET /common.css HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:35] \"GET /stats.js HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:35] \"GET /common.js HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:35] \"GET /music/racer.ogg HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:37] \"GET /images/mute.png HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:37] \"GET /images/sprites.png HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:37] \"GET /images/background.png HTTP/1.1\" 200 -\n127.0.0.1 - - [22/Jun/2020 16:58:38] code 404, message File not found\n127.0.0.1 - - [22/Jun/2020 16:58:38] \"GET /favicon.ico HTTP/1.1\" 404 -\nINIT IS TRIGGERED\n"
    }
   ],
   "source": [
    "env = RaceGameEnv()\n",
    "#env = tf_py_environment.TFPyEnvironment(env)\n",
    "#utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 500  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-8  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fc_layer_params = (200,)\n",
    "fc_layer_params = [100,50] #(150,75)\n",
    "\n",
    "##q_net = q_network.QNetwork(\n",
    "#    env.observation_spec(),\n",
    "#    env.action_spec(),\n",
    "#    fc_layer_params=fc_layer_params)\n",
    "\n",
    "conv_layer_params = [( 16 , ( 3 , 3 ), 1 ), ( 16 , ( 3 , 3 ), 1 )]\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer QNetwork is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nWARNING:tensorflow:Layer TargetQNetwork is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n"
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Return from Random agent is -999\n"
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "env.action_spec())\n",
    "\n",
    "random_return = -999\n",
    "#random_return = compute_avg_return(env, random_policy, num_eval_episodes)\n",
    "\n",
    "env._reset()\n",
    "print(\"Return from Random agent is \" + str(random_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('step_type',\n 'observation',\n 'action',\n 'policy_info',\n 'next_step_type',\n 'reward',\n 'discount')"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  #start_time = time.time()\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "  #print(\"All collect_step took %s seconds\" % (time.time() - start_time))\n",
    "  \n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for i in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PrefetchDataset shapes: (Trajectory(step_type=(64, 2), observation=(64, 2, 55, 240, 3), action=(64, 2), policy_info=(), next_step_type=(64, 2), reward=(64, 2), discount=(64, 2)), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(step_type=tf.int32, observation=tf.float64, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000001E9189DA4A8>\n"
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ntry:\\n  %%time\\nexcept:\\n  pass\\n\\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\\nagent.train = common.function(agent.train)\\n\\n# Reset the train step\\nagent.train_step_counter.assign(0)\\n\\n# Evaluate the agent\\'s policy once before training.\\nenv._reset()\\navg_return = 0\\n#avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\\nreturns = [avg_return]\\n\\nfor i in range(num_iterations):\\n\\n  # Collect a few steps using collect_policy and save to the replay buffer.\\n  for j in range(collect_steps_per_iteration):\\n    #print(\"Step \" + str(j))\\n    collect_step(env, agent.collect_policy, replay_buffer)\\n\\n  env._reset()\\n  # Sample a batch of data from the buffer and update the agent\\'s network.\\n  print(\"Agent training for %s th time\" % str(i))\\n  start_time = time.time()\\n  for j in range(collect_steps_per_iteration):\\n    experience, unused_info = next(iterator)  \\n    \\n    train_loss = agent.train(experience).loss\\n  print(\"Agent trained in %s seconds\" % (time.time() - start_time))\\n  step = agent.train_step_counter.numpy()\\n\\n  if step % log_interval == 0:\\n    print(\\'step = {0}: loss = {1}\\'.format(step, train_loss))\\n\\n  if step % eval_interval == 0:\\n    env._reset()\\n    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\\n    print(\\'step = {0}: Average Return = {1}\\'.format(step, avg_return))\\n    returns.append(avg_return)\\n'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "env._reset()\n",
    "avg_return = 0\n",
    "#avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for j in range(collect_steps_per_iteration):\n",
    "    #print(\"Step \" + str(j))\n",
    "    collect_step(env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  env._reset()\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  print(\"Agent training for %s th time\" % str(i))\n",
    "  start_time = time.time()\n",
    "  for j in range(collect_steps_per_iteration):\n",
    "    experience, unused_info = next(iterator)  \n",
    "    \n",
    "    train_loss = agent.train(experience).loss\n",
    "  print(\"Agent trained in %s seconds\" % (time.time() - start_time))\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    env._reset()\n",
    "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "step = 10: loss = 15.702394485473633\nstep = 20: loss = 18.805355072021484\nstep = 30: loss = 15.352033615112305\nstep = 40: loss = 15.260333061218262\nstep = 50: loss = 20.348064422607422\nstep = 60: loss = 24.23486328125\nstep = 70: loss = 25.89557647705078\nstep = 80: loss = 37.18161392211914\nstep = 90: loss = 30.314777374267578\nstep = 100: loss = 29.433324813842773\n"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = 0\n",
    "#avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  start_time = time.time()\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "  print(\"Agent trained in %s seconds\" % (time.time() - start_time))\n",
    "  \n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-148.7"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "avg_return = compute_avg_return(env, agent.collect_policy, num_eval_episodes)\n",
    "\n",
    "avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bittfpythonvirtualenvc896718c15ce430fbcbfe7334c839c00",
   "display_name": "Python 3.7.3 64-bit ('tfpython': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}